'''Test unified network with scene classification and place recognition'''from __future__ import print_functionimport argparseimport randomfrom os.path import join, existsfrom os import mkdirimport torchfrom torch.utils.data import DataLoaderfrom torch.nn import functional as Fimport numpy as npfrom UnifiedModel import SceneModelimport warningsimport csvparser = argparse.ArgumentParser(description='ScenePlaceRecognitionTest')parser.add_argument('--cacheBatchSize', type=int, default=4, help='Batch size for caching and testing')parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')parser.add_argument('--threads', type=int, default=4, help='Number of threads for each data loader to use')parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')parser.add_argument('--dataPath', type=str, default='data/', help='Path for centroid data.')parser.add_argument('--cachePath', type=str, default='/tmp/', help='Path to save cache to.')parser.add_argument('--resume', type=str, default='/home/ricky/ScenePlaceRecognition',                    help='Path to load checkpoint from, for resuming training or testing.')parser.add_argument('--ckpt', type=str, default='best',                    help='Resume from latest or best checkpoint.', choices=['latest', 'best'])parser.add_argument('--dataset', type=str, default='westlake', help='Dataset to use',                    choices=['westlake', 'yuquan'])parser.add_argument('-a', '--arch', type=str, default='resnet', help='the backbone network',                    choices=['resnet18', 'mobilenet_v2', 'shufflenet_v2'])# parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use',#                     choices=['netvlad', 'max', 'avg'])# parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')# parser.add_argument('--attention', action='store_true', help='Whether with the attention module.')parser.add_argument('--netVLADtrainNum', type=int, default=2, help='Number of trained blocks in the backbone.')parser.add_argument('--savePath', type=str, default='', help='where to save descriptors and classes.')def load_labels():    """    Prepare the labels, scene category relevant    """    file_name_category = 'Place365/categories_places365.txt'    classes = list()    with open(file_name_category) as class_file:        for line in class_file:            classes.append(line.strip().split(' ')[0][3:])    classes = tuple(classes)    return classesdef test(eval_set):    numDb = eval_set.numDb    test_data_loader = DataLoader(dataset=eval_set, num_workers=opt.threads,                                  batch_size=opt.cacheBatchSize, shuffle=False, pin_memory=cuda)    classes = load_labels()    with torch.no_grad():  # 不会反向传播，提高inference速度        print('====> Extracting Features')        for iteration, (input_image, indices) in enumerate(test_data_loader, 1):            input_image = input_image.to(device)            # forward inference - place add scene information            logit, dsc = unified_model.forward(input_image)            dsc = dsc.detach().cpu().numpy()            h_x = F.softmax(logit, 1).data.squeeze()            probs, index = h_x.sort(dim=1, descending=True)            del h_x, logit            probs = list(probs.detach().cpu().numpy())            index = list(index.detach().cpu().numpy())            if iteration % 50 == 0 or len(test_data_loader) <= 10:                print("==> Batch ({}/{})".format(iteration, len(test_data_loader)), flush=True)            for i in range(dsc.shape[0]):                idx = int(indices[i])                if idx < numDb:                    cls_pth = join(opt.savePath, 'database', str(idx).zfill(6) + '.csv')                    dsc_pth = join(opt.savePath, 'database', str(idx).zfill(6) + '.npy')                else:                    dsc_pth = join(opt.savePath, 'query', str(idx - numDb).zfill(6) + '.npy')                    cls_pth = join(opt.savePath, 'query', str(idx - numDb).zfill(6) + '.csv')                np.save(dsc_pth, dsc[i])                with open(cls_pth, 'w') as file:                    f_csv = csv.writer(file)                    for ii in range(5):                        l = [probs[i][ii], classes[index[i][ii]]]                        f_csv.writerow(l)            del input_image, dsc    del test_data_loaderif __name__ == "__main__":    # ignore warnings -- UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1    warnings.filterwarnings("ignore")    opt = parser.parse_args()    # designate device    cuda = not opt.nocuda    if cuda and not torch.cuda.is_available():        raise Exception("No GPU found, please run with --nocuda")    device = torch.device("cuda" if cuda else "cpu")    random.seed(opt.seed)    np.random.seed(opt.seed)    torch.manual_seed(opt.seed)    if cuda:        torch.cuda.manual_seed(opt.seed)    # designate dataset    if opt.dataset.lower() == 'yuquan':        from DataSet import Yuquan as dataset    elif opt.dataset.lower() == 'westlake':        from DataSet import WestLake as dataset    elif opt.dataset.lower() == 'gardenpoint':        from DataSet import GardenPoint as dataset    else:        raise Exception('Unknown dataset')    print('===> Loading dataset(s)')    whole_test_set = dataset.get_whole_val_set()    print('====> Query count:', whole_test_set.numQ)    # build Unified network architecture    print('===> Building model')    unified_model = SceneModel.UnifiedSceneNetwork(arch=opt.arch, num_trained=opt.netVLADtrainNum, num_clusters=64,                                                   branch1_enable=1, branch2_enable=1)    # load the paramters of the UnifiedModel branch    if opt.ckpt.lower() == 'latest':        resume_ckpt = join(opt.resume, 'checkpoints_mbnet_pitts30_n11', 'checkpoint.pth.tar')    elif opt.ckpt.lower() == 'best':        resume_ckpt = join(opt.resume, 'checkpoints_mbnet_pitts30_n11', 'model_best.pth.tar')    unified_model.load_branch2_params(resume_ckpt, 'pca.pth')    # execute test procedures    print('===> Running evaluation step')    if exists(opt.savePath):        raise RuntimeError('The saving path has existed.')    else:        mkdir(opt.savePath)        mkdir(join(opt.savePath, 'database'))        mkdir(join(opt.savePath, 'query'))    unified_model = unified_model.to(device)    unified_model.eval()    test(whole_test_set)